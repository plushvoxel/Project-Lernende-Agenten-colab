{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNNTest.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/plushvoxel/Project-Lernende-Agenten-colab/blob/master/CNNTest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "bR4Ed0mOCMzd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tarfile import open as taropen\n",
        "from urllib import request\n",
        "from struct import unpack\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, LSTM\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" #for training on gpu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iu2XmrW_CWTD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_data():\n",
        "    DataBaseURL= \"http://computer-in.love/data/{}.tar\"\n",
        "    Classes = [\"fm\", \"pager\", \"smartwares\", \"no_signal\"]\n",
        "    data = []\n",
        "    normalisationFactor = 100\n",
        "    test = 0\n",
        "\n",
        "    for modulation in Classes:\n",
        "        filename = \"{}.tar\".format(modulation)\n",
        "        request.urlretrieve(DataBaseURL.format(modulation), filename)\n",
        "        tar = taropen(filename)\n",
        "        class_label = [0]*len(Classes)\n",
        "        class_label[Classes.index(modulation)]=1\n",
        "        for member in tar.getmembers():\n",
        "            test = test + 1\n",
        "            with tar.extractfile(member) as f:\n",
        "                sample = []\n",
        "                buffer = f.read()\n",
        "                num_floats = len(buffer)//4\n",
        "                floats = unpack(\"f\"*num_floats, buffer)\n",
        "                i = floats[0::2]\n",
        "                q = floats[1::2]\n",
        "                for j in range(min(len(i), len(q))):\n",
        "                    #here happens the scaling or whatever you want\n",
        "                    sample.append([i[j]/normalisationFactor, q[j]/normalisationFactor])\n",
        "                #here happens some cross_feature if you want. just append it to the sample\n",
        "                \n",
        "                data.append([np.array(sample), np.array(class_label)])\n",
        "    random.shuffle(data)\n",
        "\n",
        "    return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U7KsvTqUHrV2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_data(data,trainP = 60, valP= 20, testP = 20):\n",
        "  pre: trainP + valP + testP <= 100\n",
        "  size =len(data)\n",
        "  trainSize = (size * trainP)//100\n",
        "  print(trainSize)\n",
        "  valSize = (size * valP)//100\n",
        "  print(valSize)\n",
        "  testSize = (size * testP)//100\n",
        "  print(testSize)\n",
        "  train = data[:trainSize]\n",
        "  val = data[trainSize: trainSize + valSize]\n",
        "  test = data[trainSize + valSize: trainSize + valSize + testSize]\n",
        "  x_train = []\n",
        "  y_train = []\n",
        "  x_val = []\n",
        "  y_val = []\n",
        "  x_test = []\n",
        "  y_test = []\n",
        "  for sample, target in train:\n",
        "    x_train.append(sample)\n",
        "    y_train.append(target)\n",
        "  for sample, target in val:\n",
        "    x_val.append(sample)\n",
        "    y_val.append(target)\n",
        "  for sample, target in test:\n",
        "    x_test.append(sample)\n",
        "    y_test.append(target)\n",
        "\n",
        "\n",
        "  return np.array(x_train), np.array(x_val), np.array(x_test), np.array(y_train), np.array(y_val), np.array(y_test) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3FWk37YIEMM3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data= create_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "owYaB7RLIbLB",
        "colab_type": "code",
        "outputId": "8e6e4c59-5627-4726-8e81-e42c3466d8b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "train_X, val_X, test_X, train_y, val_y, test_y = get_data(data)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "23888\n",
            "7962\n",
            "7962\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "b75z5VZVExlH",
        "colab_type": "code",
        "outputId": "798227f8-dff8-4ab1-f169-cf40e62ddaa7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "print(np.max(train_X[0]))\n",
        "print(val_X.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.4851842498779297\n",
            "(7962, 128, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Gm-G_8kwGXo1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_X = train_X.reshape(-1, 128, 2, 1)\n",
        "test_X = test_X.reshape(-1,128,2,1)\n",
        "val_X = val_X.reshape(-1,128,2,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y69WTN-SGkEn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "training_iters = 100 \n",
        "learning_rate = 0.001 \n",
        "batch_size = 50\n",
        "#data input (img shape: 128*2)\n",
        "n_input_1 = 128\n",
        "n_input_2 = 2\n",
        "\n",
        "# data total classes (0-9 digits)\n",
        "n_classes = 4\n",
        "tf.reset_default_graph()\n",
        "#both placeholders are of type float\n",
        "x = tf.placeholder(\"float\", [None, n_input_1,n_input_2,1])\n",
        "y = tf.placeholder(\"float\", [None, n_classes])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-fX3aLxcHkyd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def conv2d(x, W, b, strides=1):\n",
        "    # Conv2D wrapper, with bias and relu activation\n",
        "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
        "    x = tf.nn.bias_add(x, b)\n",
        "    return tf.nn.relu(x) \n",
        "\n",
        "def maxpool2d(x, k=2):\n",
        "    return tf.nn.max_pool(x, ksize=[1, k, 1, 1], strides=[1, k, 1, 1],padding='SAME')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wHgEun6_DdxA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "weights = {\n",
        "    'wc1': tf.get_variable('W0', shape=(11,2,1,8), initializer=tf.contrib.layers.xavier_initializer()), \n",
        "    'wc2': tf.get_variable('W1', shape=(11,2,8,8), initializer=tf.contrib.layers.xavier_initializer()), \n",
        "    #'wc3': tf.get_variable('W2', shape=(11,2,16,32), initializer=tf.contrib.layers.xavier_initializer()), \n",
        "    'wd1': tf.get_variable('W3', shape=(8*32*2,8), initializer=tf.contrib.layers.xavier_initializer()), \n",
        "    'out': tf.get_variable('W6', shape=(8,n_classes), initializer=tf.contrib.layers.xavier_initializer()), \n",
        "}\n",
        "biases = {\n",
        "    'bc1': tf.get_variable('B0', shape=(8), initializer=tf.contrib.layers.xavier_initializer()),\n",
        "    'bc2': tf.get_variable('B1', shape=(8), initializer=tf.contrib.layers.xavier_initializer()),\n",
        "    #'bc3': tf.get_variable('B2', shape=(32), initializer=tf.contrib.layers.xavier_initializer()),\n",
        "    'bd1': tf.get_variable('B3', shape=(8), initializer=tf.contrib.layers.xavier_initializer()),\n",
        "    'out': tf.get_variable('B4', shape=(n_classes), initializer=tf.contrib.layers.xavier_initializer()),\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fPxdB4RCOEJt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def conv_net(x, weights, biases):  \n",
        "\n",
        "    # here we call the conv2d function we had defined above and pass the input image x, weights wc1 and bias bc1.\n",
        "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
        "    # Max Pooling (down-sampling), this chooses the max value from a 2*1 matrix window and outputs a 64*2 matrix.\n",
        "    conv1 = maxpool2d(conv1, k=2)\n",
        "\n",
        "    # Convolution Layer\n",
        "    # here we call the conv2d function we had defined above and pass the input image x, weights wc2 and bias bc2.\n",
        "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
        "    # Max Pooling (down-sampling), this chooses the max value from a 2*1 matrix window and outputs a 32*2 matrix.\n",
        "    conv2 = maxpool2d(conv2, k=2)\n",
        "\n",
        "    #conv3 = conv2d(conv2, weights['wc3'], biases['bc3'])\n",
        "    # Max Pooling (down-sampling), this chooses the max value from a 2*1 matrix window and outputs a 16*2.\n",
        "    #conv3 = maxpool2d(conv3, k=2)\n",
        "\n",
        "\n",
        "    # Fully connected layer\n",
        "    # Reshape conv2 output to fit fully connected layer input\n",
        "    \n",
        "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
        "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
        "    fc1 = tf.nn.relu(fc1)\n",
        "    # Output, class prediction\n",
        "    # finally we multiply the fully connected layer with the weights and add a bias term. \n",
        "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VyZzbuNPOPQ-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pred = conv_net(x, weights, biases)\n",
        "\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=pred, labels=y))\n",
        "\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "\n",
        "#Here you check whether the index of the maximum value of the predicted image is equal to the actual labelled image. and both will be a column vector.\n",
        "correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
        "\n",
        "#calculate accuracy across all the given images and average them out. \n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "k-EQebK1Og-j",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "init = tf.global_variables_initializer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1wyWRRKcOr2S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6810
        },
        "outputId": "93314aa0-98fe-463c-8b79-80bed3d254ec"
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "    sess.run(init) \n",
        "    train_loss_list = []\n",
        "    test_loss_list = []\n",
        "    validation_loss_list = []\n",
        "    train_accuracy_list = []\n",
        "    test_accuracy_list = []\n",
        "    validation_accuracy_list = []\n",
        "    summary_writer = tf.summary.FileWriter('./Output', sess.graph)\n",
        "    for i in range(training_iters):\n",
        "        batch_num = int(len(train_X)/training_iters/batch_size)\n",
        "        for batch in range(batch_num):\n",
        "            batch_x = train_X[(i*batch_num+batch)*batch_size:min(((i*batch_num+batch)*batch_size+1)*batch_size,len(train_X))]\n",
        "            batch_y = train_y[(i*batch_num+batch)*batch_size:min(((i*batch_num+batch)*batch_size+1)*batch_size,len(train_y))]\n",
        "            # Run optimization op (backprop).\n",
        "                # Calculate batch loss and accuracy\n",
        "            opt = sess.run(optimizer, feed_dict={x: batch_x,\n",
        "                                                              y: batch_y})\n",
        "            loss, acc = sess.run([cost, accuracy], feed_dict={x: batch_x,\n",
        "                                                              y: batch_y})\n",
        "            train_loss_list.append(loss)\n",
        "            train_accuracy_list.append(acc)\n",
        "            \n",
        "        # Calculate accuracy for all 10000 mnist test images\n",
        "        val_acc,val_loss = sess.run([accuracy,cost], feed_dict={x: val_X,y : val_y})\n",
        "        validation_loss_list.append(val_loss)\n",
        "        validation_accuracy_list.append(val_acc)\n",
        "        print(\"Iter \" + str(i) + \", Loss= \" + \\\n",
        "                          \"{:.6f}\".format(train_loss_list[-1]) + \", Training Accuracy= \" + \\\n",
        "                          \"{:.5f}\".format(train_accuracy_list[-1]))\n",
        "        print(\"Validation Accuracy:\",\"{:.5f}\".format(val_acc))\n",
        "        print(\"Validation Loss:\",\"{:.5f}\".format(val_loss))\n",
        "    test_acc,test_loss = sess.run([accuracy,cost], feed_dict={x: test_X,y : test_y})\n",
        "    test_loss_list.append(test_loss)\n",
        "    test_accuracy_list.append(test_acc)\n",
        "    print(\"Testing Accuracy:\",\"{:.5f}\".format(test_acc))\n",
        "    summary_writer.close()\n",
        "    plt.plot(range(len(train_loss_list)), train_loss_list, 'black', label='Training loss')\n",
        "    plt.plot(range(len(train_loss_list)), validation_loss_list, 'red', label='Validation loss')\n",
        "    plt.title('Training and Validation loss')\n",
        "    plt.xlabel('Epochs ',fontsize=16)\n",
        "    plt.ylabel('Loss',fontsize=16)\n",
        "    plt.legend()\n",
        "    plt.figure()\n",
        "    plt.show()\n",
        "    plt.plot(range(len(train_loss_list)), train_accuracy_list, 'black', label='Training Accuracy')\n",
        "    plt.plot(range(len(train_loss_list)), validation_accuracy_list, 'red', label='Validation Accuracy')\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "    plt.xlabel('Epochs ',fontsize=16)\n",
        "    plt.ylabel('Loss',fontsize=16)\n",
        "    plt.legend()\n",
        "    plt.figure()\n",
        "    plt.show()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iter 0, Loss= 1.536821, Training Accuracy= 0.25554\n",
            "Validation Accuracy: 0.24818\n",
            "Validation Loss: 1.53769\n",
            "Iter 1, Loss= 1.486830, Training Accuracy= 0.25558\n",
            "Validation Accuracy: 0.24818\n",
            "Validation Loss: 1.49107\n",
            "Iter 2, Loss= 1.463845, Training Accuracy= 0.25383\n",
            "Validation Accuracy: 0.24818\n",
            "Validation Loss: 1.46818\n",
            "Iter 3, Loss= 1.450538, Training Accuracy= 0.25374\n",
            "Validation Accuracy: 0.24818\n",
            "Validation Loss: 1.45626\n",
            "Iter 4, Loss= 1.440044, Training Accuracy= 0.25364\n",
            "Validation Accuracy: 0.24818\n",
            "Validation Loss: 1.44623\n",
            "Iter 5, Loss= 1.434109, Training Accuracy= 0.25363\n",
            "Validation Accuracy: 0.24818\n",
            "Validation Loss: 1.44023\n",
            "Iter 6, Loss= 1.433527, Training Accuracy= 0.25313\n",
            "Validation Accuracy: 0.24818\n",
            "Validation Loss: 1.43910\n",
            "Iter 7, Loss= 1.427029, Training Accuracy= 0.25378\n",
            "Validation Accuracy: 0.24818\n",
            "Validation Loss: 1.43273\n",
            "Iter 8, Loss= 1.421691, Training Accuracy= 0.25400\n",
            "Validation Accuracy: 0.24818\n",
            "Validation Loss: 1.42724\n",
            "Iter 9, Loss= 1.418754, Training Accuracy= 0.25417\n",
            "Validation Accuracy: 0.24818\n",
            "Validation Loss: 1.42407\n",
            "Iter 10, Loss= 1.415688, Training Accuracy= 0.25412\n",
            "Validation Accuracy: 0.24818\n",
            "Validation Loss: 1.42046\n",
            "Iter 11, Loss= 1.412189, Training Accuracy= 0.25429\n",
            "Validation Accuracy: 0.24818\n",
            "Validation Loss: 1.41671\n",
            "Iter 12, Loss= 1.409447, Training Accuracy= 0.25363\n",
            "Validation Accuracy: 0.24818\n",
            "Validation Loss: 1.41345\n",
            "Iter 13, Loss= 1.406509, Training Accuracy= 0.25371\n",
            "Validation Accuracy: 0.24818\n",
            "Validation Loss: 1.41052\n",
            "Iter 14, Loss= 1.403759, Training Accuracy= 0.25375\n",
            "Validation Accuracy: 0.24818\n",
            "Validation Loss: 1.40770\n",
            "Iter 15, Loss= 1.400748, Training Accuracy= 0.25383\n",
            "Validation Accuracy: 0.24818\n",
            "Validation Loss: 1.40451\n",
            "Iter 16, Loss= 1.397615, Training Accuracy= 0.25387\n",
            "Validation Accuracy: 0.24818\n",
            "Validation Loss: 1.40100\n",
            "Iter 17, Loss= 1.394349, Training Accuracy= 0.25396\n",
            "Validation Accuracy: 0.24818\n",
            "Validation Loss: 1.39737\n",
            "Iter 18, Loss= 1.390499, Training Accuracy= 0.25420\n",
            "Validation Accuracy: 0.24818\n",
            "Validation Loss: 1.39345\n",
            "Iter 19, Loss= 1.386338, Training Accuracy= 0.25444\n",
            "Validation Accuracy: 0.24818\n",
            "Validation Loss: 1.38918\n",
            "Iter 20, Loss= 1.381652, Training Accuracy= 0.25443\n",
            "Validation Accuracy: 0.24818\n",
            "Validation Loss: 1.38425\n",
            "Iter 21, Loss= 1.376178, Training Accuracy= 0.25443\n",
            "Validation Accuracy: 0.24818\n",
            "Validation Loss: 1.37849\n",
            "Iter 22, Loss= 1.369910, Training Accuracy= 0.25406\n",
            "Validation Accuracy: 0.24818\n",
            "Validation Loss: 1.37176\n",
            "Iter 23, Loss= 1.362380, Training Accuracy= 0.25421\n",
            "Validation Accuracy: 0.24818\n",
            "Validation Loss: 1.36389\n",
            "Iter 24, Loss= 1.353845, Training Accuracy= 0.26692\n",
            "Validation Accuracy: 0.26225\n",
            "Validation Loss: 1.35489\n",
            "Iter 25, Loss= 1.340367, Training Accuracy= 0.41061\n",
            "Validation Accuracy: 0.40517\n",
            "Validation Loss: 1.34030\n",
            "Iter 26, Loss= 1.322014, Training Accuracy= 0.42933\n",
            "Validation Accuracy: 0.42401\n",
            "Validation Loss: 1.32146\n",
            "Iter 27, Loss= 1.297867, Training Accuracy= 0.30396\n",
            "Validation Accuracy: 0.30470\n",
            "Validation Loss: 1.29634\n",
            "Iter 28, Loss= 1.276651, Training Accuracy= 0.33444\n",
            "Validation Accuracy: 0.33710\n",
            "Validation Loss: 1.27405\n",
            "Iter 29, Loss= 1.254766, Training Accuracy= 0.36961\n",
            "Validation Accuracy: 0.37302\n",
            "Validation Loss: 1.25102\n",
            "Iter 30, Loss= 1.235622, Training Accuracy= 0.37992\n",
            "Validation Accuracy: 0.38244\n",
            "Validation Loss: 1.23178\n",
            "Iter 31, Loss= 1.213536, Training Accuracy= 0.37347\n",
            "Validation Accuracy: 0.37767\n",
            "Validation Loss: 1.20853\n",
            "Iter 32, Loss= 1.194190, Training Accuracy= 0.36763\n",
            "Validation Accuracy: 0.37051\n",
            "Validation Loss: 1.18864\n",
            "Iter 33, Loss= 1.175609, Training Accuracy= 0.36959\n",
            "Validation Accuracy: 0.36863\n",
            "Validation Loss: 1.16993\n",
            "Iter 34, Loss= 1.158351, Training Accuracy= 0.37496\n",
            "Validation Accuracy: 0.37553\n",
            "Validation Loss: 1.15179\n",
            "Iter 35, Loss= 1.141324, Training Accuracy= 0.38416\n",
            "Validation Accuracy: 0.38772\n",
            "Validation Loss: 1.13446\n",
            "Iter 36, Loss= 1.125833, Training Accuracy= 0.38916\n",
            "Validation Accuracy: 0.39500\n",
            "Validation Loss: 1.11799\n",
            "Iter 37, Loss= 1.111209, Training Accuracy= 0.40091\n",
            "Validation Accuracy: 0.40894\n",
            "Validation Loss: 1.10295\n",
            "Iter 38, Loss= 1.096368, Training Accuracy= 0.41901\n",
            "Validation Accuracy: 0.42464\n",
            "Validation Loss: 1.08807\n",
            "Iter 39, Loss= 1.082520, Training Accuracy= 0.43531\n",
            "Validation Accuracy: 0.44172\n",
            "Validation Loss: 1.07394\n",
            "Iter 40, Loss= 1.068708, Training Accuracy= 0.45063\n",
            "Validation Accuracy: 0.45943\n",
            "Validation Loss: 1.05996\n",
            "Iter 41, Loss= 1.055517, Training Accuracy= 0.46280\n",
            "Validation Accuracy: 0.46734\n",
            "Validation Loss: 1.04659\n",
            "Iter 42, Loss= 1.041191, Training Accuracy= 0.47307\n",
            "Validation Accuracy: 0.48066\n",
            "Validation Loss: 1.03222\n",
            "Iter 43, Loss= 1.027283, Training Accuracy= 0.48230\n",
            "Validation Accuracy: 0.49133\n",
            "Validation Loss: 1.01844\n",
            "Iter 44, Loss= 1.013357, Training Accuracy= 0.49023\n",
            "Validation Accuracy: 0.50038\n",
            "Validation Loss: 1.00475\n",
            "Iter 45, Loss= 0.999186, Training Accuracy= 0.49756\n",
            "Validation Accuracy: 0.50641\n",
            "Validation Loss: 0.99105\n",
            "Iter 46, Loss= 0.985959, Training Accuracy= 0.50371\n",
            "Validation Accuracy: 0.51256\n",
            "Validation Loss: 0.97728\n",
            "Iter 47, Loss= 0.972301, Training Accuracy= 0.50781\n",
            "Validation Accuracy: 0.51771\n",
            "Validation Loss: 0.96352\n",
            "Iter 48, Loss= 0.958551, Training Accuracy= 0.51408\n",
            "Validation Accuracy: 0.52273\n",
            "Validation Loss: 0.94956\n",
            "Iter 49, Loss= 0.944677, Training Accuracy= 0.51959\n",
            "Validation Accuracy: 0.52650\n",
            "Validation Loss: 0.93594\n",
            "Iter 50, Loss= 0.931394, Training Accuracy= 0.51572\n",
            "Validation Accuracy: 0.52173\n",
            "Validation Loss: 0.92203\n",
            "Iter 51, Loss= 0.917607, Training Accuracy= 0.71296\n",
            "Validation Accuracy: 0.71502\n",
            "Validation Loss: 0.90817\n",
            "Iter 52, Loss= 0.903525, Training Accuracy= 0.71862\n",
            "Validation Accuracy: 0.72168\n",
            "Validation Loss: 0.89428\n",
            "Iter 53, Loss= 0.889910, Training Accuracy= 0.71236\n",
            "Validation Accuracy: 0.71703\n",
            "Validation Loss: 0.88028\n",
            "Iter 54, Loss= 0.875846, Training Accuracy= 0.68929\n",
            "Validation Accuracy: 0.69442\n",
            "Validation Loss: 0.86586\n",
            "Iter 55, Loss= 0.861277, Training Accuracy= 0.69202\n",
            "Validation Accuracy: 0.69668\n",
            "Validation Loss: 0.85116\n",
            "Iter 56, Loss= 0.846541, Training Accuracy= 0.69684\n",
            "Validation Accuracy: 0.70234\n",
            "Validation Loss: 0.83592\n",
            "Iter 57, Loss= 0.830545, Training Accuracy= 0.70392\n",
            "Validation Accuracy: 0.70874\n",
            "Validation Loss: 0.81954\n",
            "Iter 58, Loss= 0.813004, Training Accuracy= 0.71486\n",
            "Validation Accuracy: 0.71766\n",
            "Validation Loss: 0.80264\n",
            "Iter 59, Loss= 0.796114, Training Accuracy= 0.72810\n",
            "Validation Accuracy: 0.72821\n",
            "Validation Loss: 0.78591\n",
            "Iter 60, Loss= 0.778464, Training Accuracy= 0.74042\n",
            "Validation Accuracy: 0.74077\n",
            "Validation Loss: 0.76866\n",
            "Iter 61, Loss= 0.761967, Training Accuracy= 0.75074\n",
            "Validation Accuracy: 0.75144\n",
            "Validation Loss: 0.75279\n",
            "Iter 62, Loss= 0.731964, Training Accuracy= 0.74660\n",
            "Validation Accuracy: 0.74956\n",
            "Validation Loss: 0.72432\n",
            "Iter 63, Loss= 0.703888, Training Accuracy= 0.75606\n",
            "Validation Accuracy: 0.75998\n",
            "Validation Loss: 0.69642\n",
            "Iter 64, Loss= 0.682233, Training Accuracy= 0.76998\n",
            "Validation Accuracy: 0.77531\n",
            "Validation Loss: 0.67485\n",
            "Iter 65, Loss= 0.660492, Training Accuracy= 0.76793\n",
            "Validation Accuracy: 0.77330\n",
            "Validation Loss: 0.65251\n",
            "Iter 66, Loss= 0.639559, Training Accuracy= 0.77387\n",
            "Validation Accuracy: 0.78121\n",
            "Validation Loss: 0.63225\n",
            "Iter 67, Loss= 0.620021, Training Accuracy= 0.78284\n",
            "Validation Accuracy: 0.78900\n",
            "Validation Loss: 0.61436\n",
            "Iter 68, Loss= 0.602847, Training Accuracy= 0.78516\n",
            "Validation Accuracy: 0.79050\n",
            "Validation Loss: 0.59618\n",
            "Iter 69, Loss= 0.585291, Training Accuracy= 0.78889\n",
            "Validation Accuracy: 0.79415\n",
            "Validation Loss: 0.57911\n",
            "Iter 70, Loss= 0.568724, Training Accuracy= 0.79534\n",
            "Validation Accuracy: 0.79879\n",
            "Validation Loss: 0.56290\n",
            "Iter 71, Loss= 0.552108, Training Accuracy= 0.79713\n",
            "Validation Accuracy: 0.80106\n",
            "Validation Loss: 0.54715\n",
            "Iter 72, Loss= 0.538152, Training Accuracy= 0.79803\n",
            "Validation Accuracy: 0.80269\n",
            "Validation Loss: 0.53208\n",
            "Iter 73, Loss= 0.523554, Training Accuracy= 0.80138\n",
            "Validation Accuracy: 0.80595\n",
            "Validation Loss: 0.51746\n",
            "Iter 74, Loss= 0.508869, Training Accuracy= 0.80387\n",
            "Validation Accuracy: 0.80733\n",
            "Validation Loss: 0.50314\n",
            "Iter 75, Loss= 0.495398, Training Accuracy= 0.80465\n",
            "Validation Accuracy: 0.80784\n",
            "Validation Loss: 0.48872\n",
            "Iter 76, Loss= 0.480951, Training Accuracy= 0.80581\n",
            "Validation Accuracy: 0.81085\n",
            "Validation Loss: 0.47452\n",
            "Iter 77, Loss= 0.466544, Training Accuracy= 0.81063\n",
            "Validation Accuracy: 0.81412\n",
            "Validation Loss: 0.46159\n",
            "Iter 78, Loss= 0.452412, Training Accuracy= 0.81482\n",
            "Validation Accuracy: 0.81412\n",
            "Validation Loss: 0.44758\n",
            "Iter 79, Loss= 0.436223, Training Accuracy= 0.81859\n",
            "Validation Accuracy: 0.81801\n",
            "Validation Loss: 0.43238\n",
            "Iter 80, Loss= 0.421907, Training Accuracy= 0.81895\n",
            "Validation Accuracy: 0.82065\n",
            "Validation Loss: 0.41816\n",
            "Iter 81, Loss= 0.408358, Training Accuracy= 0.82316\n",
            "Validation Accuracy: 0.82580\n",
            "Validation Loss: 0.40495\n",
            "Iter 82, Loss= 0.395092, Training Accuracy= 0.82638\n",
            "Validation Accuracy: 0.82643\n",
            "Validation Loss: 0.39244\n",
            "Iter 83, Loss= 0.382256, Training Accuracy= 0.82922\n",
            "Validation Accuracy: 0.83195\n",
            "Validation Loss: 0.37943\n",
            "Iter 84, Loss= 0.370957, Training Accuracy= 0.83108\n",
            "Validation Accuracy: 0.83384\n",
            "Validation Loss: 0.36721\n",
            "Iter 85, Loss= 0.358649, Training Accuracy= 0.84580\n",
            "Validation Accuracy: 0.84840\n",
            "Validation Loss: 0.35526\n",
            "Iter 86, Loss= 0.348513, Training Accuracy= 0.83558\n",
            "Validation Accuracy: 0.84137\n",
            "Validation Loss: 0.34401\n",
            "Iter 87, Loss= 0.335832, Training Accuracy= 0.85153\n",
            "Validation Accuracy: 0.85619\n",
            "Validation Loss: 0.33187\n",
            "Iter 88, Loss= 0.326178, Training Accuracy= 0.88563\n",
            "Validation Accuracy: 0.89261\n",
            "Validation Loss: 0.32104\n",
            "Iter 89, Loss= 0.318579, Training Accuracy= 0.91967\n",
            "Validation Accuracy: 0.93080\n",
            "Validation Loss: 0.31279\n",
            "Iter 90, Loss= 0.306907, Training Accuracy= 0.86250\n",
            "Validation Accuracy: 0.86323\n",
            "Validation Loss: 0.30044\n",
            "Iter 91, Loss= 0.296243, Training Accuracy= 0.90376\n",
            "Validation Accuracy: 0.91334\n",
            "Validation Loss: 0.28871\n",
            "Iter 92, Loss= 0.286869, Training Accuracy= 0.92038\n",
            "Validation Accuracy: 0.93004\n",
            "Validation Loss: 0.27909\n",
            "Iter 93, Loss= 0.276210, Training Accuracy= 0.89685\n",
            "Validation Accuracy: 0.90015\n",
            "Validation Loss: 0.26951\n",
            "Iter 94, Loss= 0.266527, Training Accuracy= 0.92568\n",
            "Validation Accuracy: 0.93406\n",
            "Validation Loss: 0.25931\n",
            "Iter 95, Loss= 0.258995, Training Accuracy= 0.94259\n",
            "Validation Accuracy: 0.94750\n",
            "Validation Loss: 0.25119\n",
            "Iter 96, Loss= 0.252908, Training Accuracy= 0.94535\n",
            "Validation Accuracy: 0.95089\n",
            "Validation Loss: 0.24357\n",
            "Iter 97, Loss= 0.243206, Training Accuracy= 0.93038\n",
            "Validation Accuracy: 0.93984\n",
            "Validation Loss: 0.23378\n",
            "Iter 98, Loss= 0.237176, Training Accuracy= 0.92315\n",
            "Validation Accuracy: 0.93507\n",
            "Validation Loss: 0.22713\n",
            "Iter 99, Loss= 0.228976, Training Accuracy= 0.94744\n",
            "Validation Accuracy: 0.95566\n",
            "Validation Loss: 0.21838\n",
            "Testing Accuracy: 0.95177\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-9721cbe71c8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0msummary_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'black'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Training loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_loss_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'red'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Validation loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training and Validation loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epochs '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfontsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   3259\u001b[0m                       mplDeprecation)\n\u001b[1;32m   3260\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3261\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3262\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3263\u001b[0m         \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwashold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1715\u001b[0m                     warnings.warn(msg % (label_namer, func.__name__),\n\u001b[1;32m   1716\u001b[0m                                   RuntimeWarning, stacklevel=2)\n\u001b[0;32m-> 1717\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1718\u001b[0m         \u001b[0mpre_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1719\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpre_doc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1370\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_alias_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1372\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1373\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m             \u001b[0mlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_grab_next_args\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    402\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mseg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    405\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mseg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    382\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_xy_from_xy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'plot'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_xy_from_xy\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m             raise ValueError(\"x and y must have same first dimension, but \"\n\u001b[0;32m--> 243\u001b[0;31m                              \"have shapes {} and {}\".format(x.shape, y.shape))\n\u001b[0m\u001b[1;32m    244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m             raise ValueError(\"x and y can be no greater than 2-D, but have \"\n",
            "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (400,) and (100,)"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeEAAAFKCAYAAAAqkecjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7ffa57498a90>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}