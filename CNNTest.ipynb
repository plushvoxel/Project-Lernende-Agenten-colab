{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNNTest.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/plushvoxel/Project-Lernende-Agenten-colab/blob/master/CNNTest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "bR4Ed0mOCMzd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tarfile import open as taropen\n",
        "from urllib import request\n",
        "from struct import unpack\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, LSTM\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" #for training on gpu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iu2XmrW_CWTD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_data(trainP = 60, valP= 20, testP = 20):\n",
        "    pre: trainP + valP + testP <= 100\n",
        "    DataBaseURL= \"http://computer-in.love/data/{}.tar\"\n",
        "    Classes = [\"fm\", \"pager\", \"smartwares\", \"no_signal\"]\n",
        "    data = []\n",
        "    normalisationFactor = 100\n",
        "    test = 0\n",
        "\n",
        "    for modulation in Classes:\n",
        "        filename = \"{}.tar\".format(modulation)\n",
        "        request.urlretrieve(DataBaseURL.format(modulation), filename)\n",
        "        tar = taropen(filename)\n",
        "        class_label = [0]*len(Classes)\n",
        "        class_label[Classes.index(modulation)]=1\n",
        "        for member in tar.getmembers():\n",
        "            test = test + 1\n",
        "            with tar.extractfile(member) as f:\n",
        "                sample = []\n",
        "                buffer = f.read()\n",
        "                num_floats = len(buffer)//4\n",
        "                floats = unpack(\"f\"*num_floats, buffer)\n",
        "                i = floats[0::2]\n",
        "                q = floats[1::2]\n",
        "                for j in range(min(len(i), len(q))):\n",
        "                    #here happens the scaling or whatever you want\n",
        "                    sample.append([i[j]/normalisationFactor, q[j]/normalisationFactor])\n",
        "                #here happens some cross_feature if you want. just append it to the sample\n",
        "                \n",
        "                data.append([np.array(sample), np.array(class_label)])\n",
        "\n",
        "    random.shuffle(data)\n",
        "    #split all data into train, val and test\n",
        "    size =len(data)\n",
        "    trainSize = (size * trainP)//100\n",
        "    print(trainSize)\n",
        "    valSize = (size * valP)//100\n",
        "    print(valSize)\n",
        "    testSize = (size * testP)//100\n",
        "    print(testSize)\n",
        "    train = data[:trainSize]\n",
        "    val = data[trainSize: trainSize + valSize]\n",
        "    test = data[trainSize + valSize: trainSize + valSize + testSize]\n",
        "    x_train = []\n",
        "    y_train = []\n",
        "    x_val = []\n",
        "    y_val = []\n",
        "    x_test = []\n",
        "    y_test = []\n",
        "    for sample, target in train:\n",
        "      x_train.append(sample)\n",
        "      y_train.append(target)\n",
        "    for sample, target in val:\n",
        "      x_val.append(sample)\n",
        "      y_val.append(target)\n",
        "    for sample, target in test:\n",
        "      x_test.append(sample)\n",
        "      y_test.append(target)\n",
        "    \n",
        "    \n",
        "    return np.array(x_train), np.array(x_val), np.array(x_test), np.array(y_train), np.array(y_val), np.array(y_test) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3FWk37YIEMM3",
        "colab_type": "code",
        "outputId": "ac632f40-3c45-4b79-b126-8b57a805f837",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "train_X, val_X, test_X, train_y, val_y, test_y = create_data()\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "189600\n",
            "63200\n",
            "63200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "b75z5VZVExlH",
        "colab_type": "code",
        "outputId": "c67ca512-bb58-4c59-9162-9e46ce42d4fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "print(np.max(train_X[0]))\n",
        "print(val_X.shape)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.23029567718505858\n",
            "(63200, 128, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Gm-G_8kwGXo1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_X = train_X.reshape(-1, 128, 2, 1)\n",
        "test_X = test_X.reshape(-1,128,2,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y69WTN-SGkEn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "training_iters = 1 \n",
        "learning_rate = 0.001 \n",
        "batch_size = 1\n",
        "#data input (img shape: 128*2)\n",
        "n_input_1 = 128\n",
        "n_input_2 = 2\n",
        "\n",
        "# data total classes (0-9 digits)\n",
        "n_classes = 4\n",
        "\n",
        "#both placeholders are of type float\n",
        "x = tf.placeholder(\"float\", [None, n_input_1,n_input_2,1])\n",
        "y = tf.placeholder(\"float\", [None, n_classes])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-fX3aLxcHkyd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def conv2d(x, W, b, strides=1):\n",
        "    # Conv2D wrapper, with bias and relu activation\n",
        "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
        "    x = tf.nn.bias_add(x, b)\n",
        "    return tf.nn.relu(x) \n",
        "\n",
        "def maxpool2d(x, k=2):\n",
        "    return tf.nn.max_pool(x, ksize=[1, k, 1, 1], strides=[1, k, 1, 1],padding='SAME')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wHgEun6_DdxA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "weights = {\n",
        "    'wc1': tf.get_variable('W0', shape=(11,2,1,8), initializer=tf.contrib.layers.xavier_initializer()), \n",
        "    'wc2': tf.get_variable('W1', shape=(11,2,8,16), initializer=tf.contrib.layers.xavier_initializer()), \n",
        "    'wc3': tf.get_variable('W2', shape=(11,2,16,32), initializer=tf.contrib.layers.xavier_initializer()), \n",
        "    'wd1': tf.get_variable('W3', shape=(32*16*2,32), initializer=tf.contrib.layers.xavier_initializer()), \n",
        "    'out': tf.get_variable('W6', shape=(32,n_classes), initializer=tf.contrib.layers.xavier_initializer()), \n",
        "}\n",
        "biases = {\n",
        "    'bc1': tf.get_variable('B0', shape=(8), initializer=tf.contrib.layers.xavier_initializer()),\n",
        "    'bc2': tf.get_variable('B1', shape=(16), initializer=tf.contrib.layers.xavier_initializer()),\n",
        "    'bc3': tf.get_variable('B2', shape=(32), initializer=tf.contrib.layers.xavier_initializer()),\n",
        "    'bd1': tf.get_variable('B3', shape=(32), initializer=tf.contrib.layers.xavier_initializer()),\n",
        "    'out': tf.get_variable('B4', shape=(n_classes), initializer=tf.contrib.layers.xavier_initializer()),\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fPxdB4RCOEJt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def conv_net(x, weights, biases):  \n",
        "\n",
        "    # here we call the conv2d function we had defined above and pass the input image x, weights wc1 and bias bc1.\n",
        "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
        "    # Max Pooling (down-sampling), this chooses the max value from a 2*1 matrix window and outputs a 64*2 matrix.\n",
        "    conv1 = maxpool2d(conv1, k=2)\n",
        "\n",
        "    # Convolution Layer\n",
        "    # here we call the conv2d function we had defined above and pass the input image x, weights wc2 and bias bc2.\n",
        "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
        "    # Max Pooling (down-sampling), this chooses the max value from a 2*1 matrix window and outputs a 32*2 matrix.\n",
        "    conv2 = maxpool2d(conv2, k=2)\n",
        "\n",
        "    conv3 = conv2d(conv2, weights['wc3'], biases['bc3'])\n",
        "    # Max Pooling (down-sampling), this chooses the max value from a 2*1 matrix window and outputs a 16*2.\n",
        "    conv3 = maxpool2d(conv3, k=2)\n",
        "\n",
        "\n",
        "    # Fully connected layer\n",
        "    # Reshape conv2 output to fit fully connected layer input\n",
        "    \n",
        "    fc1 = tf.reshape(conv3, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
        "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
        "    fc1 = tf.nn.relu(fc1)\n",
        "    # Output, class prediction\n",
        "    # finally we multiply the fully connected layer with the weights and add a bias term. \n",
        "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VyZzbuNPOPQ-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pred = conv_net(x, weights, biases)\n",
        "\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=pred, labels=y))\n",
        "\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "\n",
        "#Here you check whether the index of the maximum value of the predicted image is equal to the actual labelled image. and both will be a column vector.\n",
        "correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
        "\n",
        "#calculate accuracy across all the given images and average them out. \n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "k-EQebK1Og-j",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "init = tf.global_variables_initializer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1wyWRRKcOr2S",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "    sess.run(init) \n",
        "    train_loss = []\n",
        "    test_loss = []\n",
        "    train_accuracy = []\n",
        "    test_accuracy = []\n",
        "    summary_writer = tf.summary.FileWriter('./Output', sess.graph)\n",
        "    for i in range(training_iters):\n",
        "        for batch in range(len(train_X)//batch_size):\n",
        "            batch_x = train_X[batch*batch_size:min((batch+1)*batch_size,len(train_X))]\n",
        "            batch_y = train_y[batch*batch_size:min((batch+1)*batch_size,len(train_y))]    \n",
        "            # Run optimization op (backprop).\n",
        "                # Calculate batch loss and accuracy\n",
        "            opt = sess.run(optimizer, feed_dict={x: batch_x,\n",
        "                                                              y: batch_y})\n",
        "            loss, acc = sess.run([cost, accuracy], feed_dict={x: batch_x,\n",
        "                                                              y: batch_y})\n",
        "        print(\"Iter \" + str(i) + \", Loss= \" + \\\n",
        "                      \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
        "                      \"{:.5f}\".format(acc))\n",
        "        print(\"Optimization Finished!\")\n",
        "\n",
        "        # Calculate accuracy for all 10000 mnist test images\n",
        "        print(test_X.shape)\n",
        "        print(test_y.shape)\n",
        "        test_acc,valid_loss = sess.run([accuracy,cost], feed_dict={x: test_X,y : test_y})\n",
        "        train_loss.append(loss)\n",
        "        test_loss.append(valid_loss)\n",
        "        train_accuracy.append(acc)\n",
        "        test_accuracy.append(test_acc)\n",
        "        print(\"Testing Accuracy:\",\"{:.5f}\".format(test_acc))\n",
        "    summary_writer.close()\n",
        "    plt.plot(range(len(train_loss)), train_loss, 'b', label='Training loss')\n",
        "    plt.plot(range(len(train_loss)), test_loss, 'r', label='Test loss')\n",
        "    plt.title('Training and Test loss')\n",
        "    plt.xlabel('Epochs ',fontsize=16)\n",
        "    plt.ylabel('Loss',fontsize=16)\n",
        "    plt.legend()\n",
        "    plt.figure()\n",
        "    plt.show()\n",
        "    plt.plot(range(len(train_loss)), train_accuracy, 'b', label='Training Accuracy')\n",
        "    plt.plot(range(len(train_loss)), test_accuracy, 'r', label='Test Accuracy')\n",
        "    plt.title('Training and Test Accuracy')\n",
        "    plt.xlabel('Epochs ',fontsize=16)\n",
        "    plt.ylabel('Loss',fontsize=16)\n",
        "    plt.legend()\n",
        "    plt.figure()\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}